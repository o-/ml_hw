## Solutions to question 6

The solutions for question 6 can all be generated by the `./lr.py` python script.

The python script includes two classes `SG` which implements stochastic gradient descent and
`ClosedFormSolution` which implements a closed form solution using numpy's matrix inversion.
Each of them takes hyperparameters as constructor arguments and implements a `run` function
to find a solution.

Both classes support ridge regression, for standard regression the ridge factor is set to 0.

The class `NormalizedInputs` takes a series of values and a model, generates inputs for the
optimizations using the model and finally normalizes the columns [0..1]. Every row gets an
additional `1.0` entry, for the bias term.

Around that there is a main function that acts as a driver. Depending on the command line
parameters it will instantiate several problems and, search for a solution, evaluate the
error rates, try different ridge factors, or plot the results.

The answers for part (b) are generated by the `gen6b.sh` script, the expected output (minus
the graphs) is shown in `6b.log`.

Similarly `gen6c.sh` generates solutions for question (c), the output is in `6c.log`
The optimal λ,θ,test and training results under k-cross validation are listed in
the output log. Also, as can be seen by the comparisons at the end of the log,
n=3 provides the best approximations in general and λ seems convex.
